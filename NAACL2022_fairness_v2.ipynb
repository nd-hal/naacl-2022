{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rtgbrM76oD9s"
   },
   "source": [
    "# ACL 2022 Submission: Fairness Calculations\n",
    "\n",
    "The input required here is the merged prediction files, and the gold standard data with the relevant demographic information.\n",
    "\n",
    "Output is a spreadsheet with calculated intersectional DI scores.\n",
    "\n",
    "## Note\n",
    "\n",
    "This jupyter notebook is for demonstration purposes. \n",
    "The *generate_plots.R* file is the main file for reproducing the results in the paper.\n",
    "This file is included as a reference for calculating the various metrics, but will need to be adapted to suit the user's needs (e.g., checking whether model predictions are correct/incorrect).\n",
    "\n",
    "## Setup\n",
    "\n",
    "First the following upgrades are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "75mLLOyW_lBE",
    "outputId": "8e7ad4be-61a8-4e88-bab8-465b34ce3473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openpyxl in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/lalor/miniconda3/envs/naacl2022/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade openpyxl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TO6IMa8vE5aW"
   },
   "source": [
    "## Restart runtime after executing the above cell to get the latest version of pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-meESHofN0uc"
   },
   "source": [
    "# Psychometric and FIPI Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljhNFRHEytvn",
    "outputId": "74e299cb-6ae3-46ea-ab4b-92d4aeaf7b87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "2023-10-26 08:56:58.658471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 08:56:59.532990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Columns (25,26,27,36,37,38,41,53,54,55,56,57,58,59,60,61,62,126,127,128,130,131,133,134,136) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age_Gender', 'Age_Gender_Race', 'Age_Gender_Race_Education', 'Age_Gender_Race_Income', 'Age_Gender_Education', 'Age_Gender_Education_Income', 'Age_Gender_Income', 'Age_Race', 'Age_Race_Education', 'Age_Race_Education_Income', 'Age_Race_Income', 'Age_Education', 'Age_Education_Income', 'Age_Income', 'Gender_Race', 'Gender_Race_Education', 'Gender_Race_Education_Income', 'Gender_Race_Income', 'Gender_Education', 'Gender_Education_Income', 'Gender_Income', 'Race_Education', 'Race_Education_Income', 'Race_Income', 'Education_Income', 'Age_Gender_Race_Education_Income']\n"
     ]
    }
   ],
   "source": [
    "# load libraries and data\n",
    "# to replicate on google colab just drag and drop relevant files into the directory (they do not persist)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "\n",
    "from aif360.sklearn.metrics import disparate_impact_ratio\n",
    "from aif360.datasets import StandardDataset\n",
    "\n",
    "# need this for psychometric data, the other ones have demographics in the files\n",
    "test_data_psych = pd.read_excel(\"PsychometricData.xlsx\", skiprows=lambda x: x in [1], header=0)\n",
    "test_data_fipi = pd.read_csv(\"fipi.csv\")\n",
    "HS_train = pd.read_csv(\"trainHS.csv\")\n",
    "HS_valid = pd.read_csv(\"validHS.csv\")\n",
    "HS_test = pd.read_csv(\"testHS.csv\")\n",
    "frames = [HS_train, HS_valid, HS_test]\n",
    "test_data_HS = pd.concat(frames)\n",
    "\n",
    "test_data_psych = test_data_psych[[\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\n",
    "            \"Text_TrustPhys\",\"Label_SubjectiveLit\",\"Label_TrustPhys\",\n",
    "            \"Label_Anxiety\",\"Label_Numeracy\",\"D1\",\n",
    "            \"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\n",
    "            ]]\n",
    "\n",
    "test_data_fipi = test_data_fipi[[\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\n",
    "            \"Text_TrustPhys\",\"Label_SubjectiveLit\",\"Label_TrustPhys\",\n",
    "            \"Label_Anxiety\",\"Label_Numeracy\",\"D1\",\n",
    "            \"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\n",
    "            ]]\n",
    "\n",
    "test_data_HS = test_data_HS[[\"text\", \"gender\", \"age\",\"country\",\"ethnicity\",\"label\"]]\n",
    "\n",
    "\n",
    "test_data_fipi[['D1', 'D2', 'D3', 'D4', 'D5']] = test_data_fipi[['D1', 'D2', 'D3', 'D4', 'D5']].apply(pd.to_numeric, errors='coerce')\n",
    "test_data_psych[['D1', 'D2', 'D3', 'D4', 'D5']] = test_data_psych[['D1', 'D2', 'D3', 'D4', 'D5']].apply(pd.to_numeric, errors='coerce')\n",
    "test_data_HS[[\"gender\", \"age\",\"ethnicity\",\"label\"]] = test_data_HS[[\"gender\", \"age\",\"ethnicity\",\"label\"]].apply(pd.to_numeric, errors='coerce')\n",
    "test_data_psych.dropna(subset=['D1', 'D2', 'D3', 'D4', 'D5'], inplace=True)\n",
    "test_data_fipi.dropna(subset=['D1', 'D2', 'D3', 'D4', 'D5'], inplace=True)\n",
    "test_data_HS.dropna(subset=[\"gender\", \"age\",\"ethnicity\",\"label\"], inplace=True)\n",
    "\n",
    "test_data_HS.rename(\n",
    "    columns={\n",
    "        \"gender\":\"Gender_bin\",\n",
    "        \"age\":\"Age_bin\",\n",
    "        \"ethnicity\":\"Race_bin\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "test_data_HS = test_data_HS[~test_data_HS.text.str.contains(\"user user user\")]\n",
    "\"\"\"\n",
    "### Demographic binarization assumptions\n",
    "\n",
    "- D1 (Age): Over/under 55\n",
    "- D2 (Gender): already binarized\n",
    "- D3 (Race): White/non-White\n",
    "- D4 (Education): College grad or higher yes/no\n",
    "- D5 (Income): $55k+ yes/no \n",
    "\"\"\"\n",
    "\n",
    "# first binarize all of our columns.\n",
    "test_data_fipi[\"Age_bin\"] = (test_data_fipi[\"D1\"] <= 38).astype(int)\n",
    "test_data_fipi[\"Gender_bin\"] = (test_data_fipi[\"D2\"] == 1).astype(int)\n",
    "test_data_fipi[\"Race_bin\"] = (test_data_fipi[\"D3\"] == 1).astype(int)\n",
    "test_data_fipi[\"Education_bin\"] = (test_data_fipi[\"D4\"] >= 5).astype(int)\n",
    "test_data_fipi[\"Income_bin\"] = (test_data_fipi[\"D5\"] >= 4).astype(int)\n",
    "\n",
    "test_data_psych[\"Age_bin\"] = (test_data_psych[\"D1\"] <= 38).astype(int)\n",
    "test_data_psych[\"Gender_bin\"] = (test_data_psych[\"D2\"] == 1).astype(int)\n",
    "test_data_psych[\"Race_bin\"] = (test_data_psych[\"D3\"] == 1).astype(int)\n",
    "test_data_psych[\"Education_bin\"] = (test_data_psych[\"D4\"] >= 5).astype(int)\n",
    "test_data_psych[\"Income_bin\"] = (test_data_psych[\"D5\"] >= 4).astype(int)\n",
    "\n",
    "# I should be able to calculate all intersections programmatically. \n",
    "\n",
    "column_names = [\"Age_bin\", \"Gender_bin\", \"Race_bin\", \"Education_bin\", \"Income_bin\"]\n",
    "\n",
    "combinations = []\n",
    "# two-way \n",
    "for aa in range(len(column_names)):\n",
    "  a = column_names[aa]\n",
    "  for bb in range(aa + 1, len(column_names)):\n",
    "    b = column_names[bb]\n",
    "    if a == b:\n",
    "      continue \n",
    "    combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0])\n",
    "    for cc in range(bb + 1, len(column_names)):\n",
    "      c = column_names[cc]\n",
    "      if a == b or a == c or b == c:\n",
    "        continue \n",
    "      combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0] + \"_\" + c.split(\"_\")[0])\n",
    "      for dd in range(cc + 1, len(column_names)):\n",
    "        d = column_names[dd]  \n",
    "        if a == b or a == c or a == d or b == c or b == d or c == d:\n",
    "          continue \n",
    "        combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0] + \"_\" + c.split(\"_\")[0] + \"_\" + d.split(\"_\")[0])\n",
    "      \n",
    "combinations.append(\"Age_Gender_Race_Education_Income\")\n",
    "\n",
    "print(combinations)\n",
    "\n",
    "# this will be a fraction: what percentage of categories is someone in the privileged class? \n",
    "\n",
    "\n",
    "for comb in combinations:\n",
    "  columns = [a + \"_bin\" for a in comb.split(\"_\")]\n",
    "  test_data_psych[comb] = 0\n",
    "  for i in range(len(columns)):\n",
    "    test_data_psych[comb] += test_data_psych[columns[i]].astype(float)\n",
    "  test_data_psych[comb] = test_data_psych[comb] / len(columns) \n",
    "\n",
    "  test_data_fipi[comb] = 0\n",
    "  for i in range(len(columns)):\n",
    "    test_data_fipi[comb] += test_data_fipi[columns[i]].astype(float)\n",
    "  test_data_fipi[comb] = test_data_fipi[comb] / len(columns) \n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NJYava8MjS-W"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4SVjW-J_qPH8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generatePlots_v3(basefile, modelTask, textCol, full_N=False, gold_ratio=False):\n",
    "    frames = []\n",
    "\n",
    "    fname = basefile\n",
    "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
    "    df = bert_preds\n",
    "\n",
    "    df[\"s1\"] = df[\"sentence\"]\n",
    "    if \"FIPI\" in basefile:\n",
    "      test_data = test_data_fipi\n",
    "    else:\n",
    "      test_data = test_data_psych\n",
    "    test_data[\"s1\"] = test_data[textCol]\n",
    "    #print(textCol)\n",
    "    #print(test_data.head())\n",
    "    #print(basefile, fname)\n",
    "    D = df.merge(test_data, on=\"s1\", how=\"inner\")\n",
    "    #print(\"start: \" + str(len(df[\"s1\"])))\n",
    "    #print(\"merge: \" + str(len(D[\"s1\"])))\n",
    "\n",
    "    try:\n",
    "      D[\"probs\"] = D[\"preds\"]\n",
    "    except:\n",
    "      try:\n",
    "        D[\"probs\"] = D[\"pred\"]\n",
    "      except:\n",
    "        #try:\n",
    "        D[\"probs\"] = D[\"probs\"].apply(lambda x: float(eval(x)[0]))\n",
    "        #except:\n",
    "        #  print(basefile)\n",
    "        #  raise Exception(\"No preds column found\")\n",
    "\n",
    "    # For continuous, we'll use the stated label\n",
    "    # calculate median and use that as cutoff instead of 0.5 \n",
    "    if \"Continuous\" in basefile:\n",
    "      labelColumn = textCol.replace(\"Text\", \"Label\")\n",
    "      median_val = statistics.median(D[labelColumn])\n",
    "      try:\n",
    "        newVar = D[\"label\"]\n",
    "      except:\n",
    "        D[\"label\"] = D[labelColumn]\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
    "      D[\"label_binarized\"] = D[\"label\"].apply(lambda x: 0 if x < median_val else 1)\n",
    "      #print(median_val)\n",
    "    else:\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "      D[\"label_binarized\"] = D[\"label\"]\n",
    "\n",
    "    D2 = D\n",
    "\n",
    "    demogColumns = [\n",
    "                    \"Age_bin\", \"Gender_bin\", \"Race_bin\",\n",
    "        \"Education_bin\", \"Income_bin\", \"Age_Gender\", \"Age_Gender_Race\",\n",
    "        \"Age_Gender_Race_Education\", \"Age_Gender_Race_Income\",\n",
    "        \"Age_Gender_Education\", \"Age_Gender_Education_Income\",\n",
    "        \"Age_Gender_Income\", \"Age_Race\", \"Age_Race_Education\",\n",
    "        \"Age_Race_Education_Income\", \"Age_Race_Income\", \"Age_Education\",\n",
    "        \"Age_Education_Income\", \"Age_Income\", \"Gender_Race\",\n",
    "        \"Gender_Race_Education\", \"Gender_Race_Education_Income\",\n",
    "        \"Gender_Race_Income\", \"Gender_Education\", \"Gender_Education_Income\",\n",
    "        \"Gender_Income\", \"Race_Education\", \"Race_Education_Income\",\n",
    "        \"Race_Income\", \"Education_Income\", \"Age_Gender_Race_Education_Income\"\n",
    "    ]\n",
    "    DIs = []\n",
    "    demog_trues = []\n",
    "    FVs = []\n",
    "\n",
    "    # laplace smoothing to account for zeros\n",
    "    for dc in demogColumns:\n",
    "      \n",
    "      positive_predictions = D2[\"probs_binarized\"]==1\n",
    "      positive_gold = D2[\"label_binarized\"]==1\n",
    "      protected = D2[dc]==0\n",
    "      privileged = D2[dc] == 1\n",
    "      N = 2\n",
    "      alpha = 1\n",
    "\n",
    "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
    "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
    "\n",
    "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
    "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
    "\n",
    "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
    "\n",
    "\n",
    "      try:\n",
    "        DI = DI_numerator / DI_denominator\n",
    "        ytrue = ytrue_numerator / ytrue_denominator\n",
    "        FV = np.abs(DI_numerator - ypred_global)\n",
    "      except:\n",
    "        DI=0\n",
    "        ytrue=0\n",
    "        FV=0\n",
    "      \n",
    "      if gold_ratio: \n",
    "          DI = DI / ytrue\n",
    "      \n",
    "      DIs.append(DI) \n",
    "      FVs.append(FV)\n",
    "      demog_trues.append((dc, ytrue))\n",
    "\n",
    "    # auc from sklearn\n",
    "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # other metrics: MSE, pearson, F1\n",
    "    mse = mean_squared_error(D2[\"label\"], D2[\"probs\"])\n",
    "    pearsonscore, prob = pearsonr(D2[\"label\"], D2[\"probs\"])\n",
    "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
    "\n",
    "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
    "\n",
    "\n",
    "\n",
    "def get_results(full_N, gold_ratio, models):\n",
    "    DIs, aucs, xaucs = [], [], []\n",
    "    task = []\n",
    "    dc_tracker = []\n",
    "    l = []\n",
    "    results = []\n",
    "    demog_trues = []\n",
    "\n",
    "    for basefname in models:\n",
    "      for i in range(len(modelTasks)):\n",
    "        m = modelTasks[i]\n",
    "        l.append(m) \n",
    "        #print(i, textCols)\n",
    "        colname = textCols[i]\n",
    "        #print(basefname, m, colname)\n",
    "        if m.lower() not in basefname.lower():\n",
    "          #if 'FIPI' in basefname and m.lower() ==\"subjectivelit\":\n",
    "          #  m = \"SubjectiveLit\"\n",
    "          #else:\n",
    "            continue\n",
    "        outs, dt = generatePlots_v3(basefname, m, colname, full_N, gold_ratio)\n",
    "        if len(demog_trues) == 0:\n",
    "          demog_trues.extend(dt)\n",
    "        modelname = basefname.split(\"/\")[1]\n",
    "        results.append([modelname, m] + outs)\n",
    "\n",
    "    colnames = [\"model\", \n",
    "              \"DV\", \n",
    "              \"MSE\", \"Pearson R\", \"F1\",\n",
    "              \"AUC\", \n",
    "        \"DI_Age_bin\", \"DI_Gender_bin\", \"DI_Race_bin\",\n",
    "        \"DI_Education_bin\", \"DI_Income_bin\", \"DI_Age_Gender\", \"DI_Age_Gender_Race\",\n",
    "        \"DI_Age_Gender_Race_Education\", \"DI_Age_Gender_Race_Income\",\n",
    "        \"DI_Age_Gender_Education\", \"DI_Age_Gender_Education_Income\",\n",
    "        \"DI_Age_Gender_Income\", \"DI_Age_Race\", \"DI_Age_Race_Education\",\n",
    "        \"DI_Age_Race_Education_Income\", \"DI_Age_Race_Income\", \"DI_Age_Education\",\n",
    "        \"DI_Age_Education_Income\", \"DI_Age_Income\", \"DI_Gender_Race\",\n",
    "        \"DI_Gender_Race_Education\", \"DI_Gender_Race_Education_Income\",\n",
    "        \"DI_Gender_Race_Income\", \"DI_Gender_Education\", \"DI_Gender_Education_Income\",\n",
    "        \"DI_Gender_Income\", \"DI_Race_Education\", \"DI_Race_Education_Income\",\n",
    "        \"DI_Race_Income\", \"DI_Education_Income\", \"DI_Age_Gender_Race_Education_Income\",\n",
    "        \"FV_Age_bin\", \"FV_Gender_bin\", \"FV_Race_bin\",\n",
    "        \"FV_Education_bin\", \"FV_Income_bin\", \"FV_Age_Gender\", \"FV_Age_Gender_Race\",\n",
    "        \"FV_Age_Gender_Race_Education\", \"FV_Age_Gender_Race_Income\",\n",
    "        \"FV_Age_Gender_Education\", \"FV_Age_Gender_Education_Income\",\n",
    "        \"FV_Age_Gender_Income\", \"FV_Age_Race\", \"FV_Age_Race_Education\",\n",
    "        \"FV_Age_Race_Education_Income\", \"FV_Age_Race_Income\", \"FV_Age_Education\",\n",
    "        \"FV_Age_Education_Income\", \"FV_Age_Income\", \"FV_Gender_Race\",\n",
    "        \"FV_Gender_Race_Education\", \"FV_Gender_Race_Education_Income\",\n",
    "        \"FV_Gender_Race_Income\", \"FV_Gender_Education\", \"FV_Gender_Education_Income\",\n",
    "        \"FV_Gender_Income\", \"FV_Race_Education\", \"FV_Race_Education_Income\",\n",
    "        \"FV_Race_Income\", \"FV_Education_Income\", \"FV_Age_Gender_Race_Education_Income\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results) \n",
    "    df.columns = colnames\n",
    "\n",
    "    return df, demog_trues\n",
    "\n",
    "def calculate_fairness(infile, outfile, weighted=False, unionYN=False):\n",
    "\n",
    "  models = [infile]\n",
    "\n",
    "  output, demogs = get_results(\n",
    "    unionYN,\n",
    "    weighted, \n",
    "    models\n",
    "  )\n",
    "\n",
    "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
    "  wordlists = wordlists.split(\".\")[0]\n",
    "  output[\"model\"] = outfile\n",
    "  output[\"adjustedDI\"] = weighted\n",
    "  output[\"debiasing\"] = debiasing\n",
    "  output[\"wordlists\"] = wordlists\n",
    "  output[\"fullN\"] = unionYN\n",
    "\n",
    "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
    "  demog_df[\"model\"] = outfile\n",
    "  demog_df[\"ratio\"] = weighted\n",
    "\n",
    "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwUjw75w-N3X",
    "outputId": "bede23aa-5c2e-4b1e-89ac-d5ad169c3a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8302\n",
      "start: 8395\n",
      "merge: 8302\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8301\n",
      "start: 8395\n",
      "merge: 8301\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8302\n",
      "start: 8395\n",
      "merge: 8302\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8366\n",
      "start: 8395\n",
      "merge: 8301\n",
      "start: 8395\n",
      "merge: 8301\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8396\n",
      "start: 8484\n",
      "merge: 8396\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8392\n",
      "start: 8484\n",
      "merge: 8392\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8397\n",
      "start: 8484\n",
      "merge: 8397\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8450\n",
      "start: 8484\n",
      "merge: 8392\n",
      "start: 8484\n",
      "merge: 8392\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8408\n",
      "start: 8502\n",
      "merge: 8408\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8407\n",
      "start: 8502\n",
      "merge: 8407\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8409\n",
      "start: 8502\n",
      "merge: 8409\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8468\n",
      "start: 8502\n",
      "merge: 8407\n",
      "start: 8502\n",
      "merge: 8407\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8403\n",
      "start: 8498\n",
      "merge: 8403\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8402\n",
      "start: 8498\n",
      "merge: 8402\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8404\n",
      "start: 8498\n",
      "merge: 8404\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8464\n",
      "start: 8498\n",
      "merge: 8402\n",
      "start: 8498\n",
      "merge: 8402\n"
     ]
    }
   ],
   "source": [
    "modelTasks = [\"Anxiety\", \"Numeracy\", \"SubjectiveLit\", \"TrustPhys\"]\n",
    "\n",
    "textCols = [\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\"Text_TrustPhys\"]\n",
    "\n",
    "labelCols = [\"Label_Anxiety\",\"Label_Numeracy\",\"Label_SubjectiveLit\",\"Label_TrustPhys\"]\n",
    "taskNames = [\n",
    "            'Psychometric_Anxiety',\n",
    "            'Psychometric_Numeracy',\n",
    "            'Psychometric_SubjectiveLit',\n",
    "            'Psychometric_TrustPhys'\n",
    "            #'FIPI_Extraverted',\n",
    "            #'FIPI_Stable'\n",
    "]\n",
    "\n",
    "debiasing = [\n",
    "    \"PT\",\n",
    "    \"PTD\"\n",
    "#    \"PTDCDA\",\n",
    "#    \"PTDDropout\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"Continuous\",\n",
    "    \"Binary\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    \"BERT\",\n",
    "    \"RoBERTa\",\n",
    "    \"CNN\"\n",
    "]\n",
    "\n",
    "\n",
    "for m in taskNames:\n",
    "    for d in debiasing:\n",
    "        for t in tasks:\n",
    "          for mm in models:\n",
    "                try:\n",
    "                    fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
    "                    #print(fname)\n",
    "                    outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
    "                    calculate_fairness(fname, outname, weighted=False, unionYN=False)\n",
    "                    outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
    "                    calculate_fairness(fname, outname, weighted=True, unionYN=False)\n",
    "                except:\n",
    "                    print(\"error\", fname)\n",
    "                    raise Exception()\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5JrHNWrcN5FL"
   },
   "source": [
    "# Ask a Patient Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BZ0fVsybNV7V"
   },
   "outputs": [],
   "source": [
    "def generatePlots_AAP(basefile, full_N=False, gold_ratio=False):\n",
    "    frames = []\n",
    "\n",
    "    fname = basefile\n",
    "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
    "    D = bert_preds\n",
    "    df = D\n",
    "\n",
    "    if \"CNN\" in fname:\n",
    "      bertfile = fname.replace(\"CNN\", \"BERT\")\n",
    "      test_data = pd.read_csv(bertfile, quotechar=\"\\\"\", encoding=\"utf-8\")\n",
    "      D = df.merge(test_data, on=\"sentence\", how=\"inner\")\n",
    "      #print(\"start: \" + str(len(df[\"sentence\"])))\n",
    "      #print(\"merge: \" + str(len(D[\"sentence\"])))\n",
    "      try:\n",
    "        D[\"probs\"] = D[\"preds\"]\n",
    "      except:\n",
    "        D[\"probs\"] = D[\"pred\"]\n",
    "    else:\n",
    "      try:\n",
    "        D[\"probs\"] = D[\"preds\"]\n",
    "      except:\n",
    "        D[\"probs\"] = D[\"pred\"]\n",
    "\n",
    "    # For continuous, we'll use the stated label\n",
    "    # calculate median and use that as cutoff instead of 0.5 \n",
    "    #median_val = 0.5\n",
    "    median_val = statistics.median(D[\"label\"])\n",
    "\n",
    "    D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
    "    D[\"label_binarized\"] = D[\"label\"].apply(lambda x: 0 if x < median_val else 1)\n",
    "    #print(median_val)\n",
    "    D2 = D.dropna()\n",
    "\n",
    "    demogColumns = [\n",
    "                    \"Age\", \"Gender\", \"Age_Gender\"\n",
    "    ]\n",
    "    DIs = []\n",
    "    FVs=[]\n",
    "    demog_trues = []\n",
    "    D2[\"Age\"] = (D2[\"x2\"] <= 56).astype(int)\n",
    "    D2[\"Gender\"] = (D2[\"x1\"] == \"M\").astype(int)\n",
    "    D2[\"Age_Gender\"] = (D2[\"Age\"].astype(int) + D2[\"Gender\"].astype(int)) / 2\n",
    "    \n",
    "\n",
    "    for dc in demogColumns:\n",
    "      \n",
    "      positive_predictions = D2[\"probs_binarized\"]==1\n",
    "      positive_gold = D2[\"label_binarized\"]==1\n",
    "      protected = D2[dc]==0\n",
    "      privileged = D2[dc] == 1\n",
    "      N = 2\n",
    "      alpha = 1\n",
    "      \n",
    "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
    "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
    "\n",
    "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
    "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
    "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
    "\n",
    "      DI = DI_numerator / DI_denominator\n",
    "      ytrue = ytrue_numerator / ytrue_denominator\n",
    "      FV = np.abs(DI_numerator - ypred_global)\n",
    "      \n",
    "      if gold_ratio: \n",
    "          DI = DI / ytrue\n",
    "      #print(dc)\n",
    "\n",
    "      \n",
    "      DIs.append(DI) \n",
    "      FVs.append(FV)\n",
    "      demog_trues.append((dc, ytrue))\n",
    "\n",
    "    # auc from sklearn\n",
    "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    mse = mean_squared_error(D2[\"label\"], D2[\"probs\"])\n",
    "    pearsonscore, prob = pearsonr(D2[\"label\"], D2[\"probs\"])\n",
    "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
    "\n",
    "    return [mse, pearsonscore, f1score, auc] + DIs+FVs, demog_trues\n",
    "\n",
    "\n",
    "def get_results_AAP(full_N, gold_ratio, models):\n",
    "    DIs, aucs, xaucs = [], [], []\n",
    "    task = []\n",
    "    dc_tracker = []\n",
    "    l = []\n",
    "    results = []\n",
    "    demog_trues = []\n",
    "\n",
    "    for basefname in models:\n",
    "        outs, dt = generatePlots_AAP(basefname, full_N, gold_ratio)\n",
    "        if len(demog_trues) == 0:\n",
    "          demog_trues.extend(dt)\n",
    "        modelname = basefname.split(\"/\")[1]\n",
    "        results.append([modelname, m] + outs)\n",
    "\n",
    "    colnames = [\"model\", \n",
    "              \"DV\", \n",
    "              \"MSE\", \"Pearson R\", \"F1\",\n",
    "              \"AUC\", \n",
    "        \"DI_Age\", \"DI_Gender\", \"DI_Age_Gender\",\n",
    "        \"FV_Age\", \"FV_Gender\", \"FV_Age_Gender\", \n",
    "    ]\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results) \n",
    "    df.columns = colnames\n",
    "\n",
    "    return df, demog_trues\n",
    "\n",
    "def calculate_fairness_AAP(infile, outfile, weighted=False, unionYN=False):\n",
    "\n",
    "  models = [infile]\n",
    "\n",
    "  output, demogs = get_results_AAP(\n",
    "    unionYN,\n",
    "    weighted, \n",
    "    models\n",
    "  )\n",
    "\n",
    "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
    "  wordlists = wordlists.split(\".\")[0]\n",
    "  output[\"model\"] = outfile\n",
    "  output[\"adjustedDI\"] = weighted\n",
    "  output[\"debiasing\"] = debiasing\n",
    "  output[\"wordlists\"] = wordlists\n",
    "  output[\"fullN\"] = unionYN\n",
    "\n",
    "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
    "  demog_df[\"model\"] = outfile\n",
    "  demog_df[\"ratio\"] = weighted\n",
    "\n",
    "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXjevffENJyn",
    "outputId": "a06b2b36-8c1c-4d4a-8a68-f307ff627a7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n",
      "No negative samples in y_true, false positive value should be meaningless\n"
     ]
    }
   ],
   "source": [
    "\n",
    "taskNames = [\n",
    "            'AskAPatient_AskAPatient'\n",
    "]\n",
    "\n",
    "debiasing = [\n",
    "    \"PT\",\n",
    "    \"PTD\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "    \"Continuous\",\n",
    "    \"Binary\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    \"BERT\",\n",
    "    \"RoBERTa\",\n",
    "#    \"CNN\"\n",
    "]\n",
    "\n",
    "\n",
    "for m in taskNames:\n",
    "    for d in debiasing:\n",
    "        for t in tasks:\n",
    "          for mm in models:\n",
    "            try:\n",
    "                fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
    "                outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
    "                calculate_fairness_AAP(fname, outname, weighted=False, unionYN=False)\n",
    "                outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
    "                calculate_fairness_AAP(fname, outname, weighted=True, unionYN=False)\n",
    "            except:\n",
    "                print(\"error\",fname)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B8CUDxovCLC7"
   },
   "source": [
    "# Hate Speech Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gl6Mo4pKCMbM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generatePlots_HS(basefile, full_N=False, gold_ratio=False):\n",
    "    frames = []\n",
    "\n",
    "    fname = basefile\n",
    "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
    "    df = bert_preds\n",
    "\n",
    "    df[\"s1\"] = df[\"sentence\"]\n",
    "    test_data = test_data_HS\n",
    "    test_data[\"s1\"] = test_data[\"text\"]\n",
    "    D = df.merge(test_data, on=\"s1\", how=\"inner\")\n",
    "    D.drop_duplicates(subset=[\"s1\"], inplace=True)\n",
    "    D[\"label\"] = D[\"label_y\"]\n",
    "    #print(\"start: \" + str(len(df[\"s1\"])))\n",
    "    #print(\"merge: \" + str(len(D[\"s1\"])))\n",
    "\n",
    "    try:\n",
    "      D[\"probs\"] = D[\"preds\"]\n",
    "    except:\n",
    "      try:\n",
    "        D[\"probs\"] = D[\"pred\"]\n",
    "      except:\n",
    "        D[\"probs\"] = D[\"probs\"]\n",
    "\n",
    "    # For continuous, we'll use the stated label\n",
    "    # calculate median and use that as cutoff instead of 0.5 \n",
    "    median_val = 0.5\n",
    "    try:\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if float(x[1:-1]) < median_val else 1)\n",
    "    except:\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if float(x) < median_val else 1) \n",
    "    D[\"probs\"] = D[\"probs_binarized\"]\n",
    "    D[\"label_binarized\"] = D[\"label\"]\n",
    "    #print(median_val)\n",
    "    D2 = D.dropna()\n",
    "\n",
    "    demogColumns = [\n",
    "                    \"Age_bin\", \"Gender_bin\", \"Race_bin\",\n",
    "        \"Age_Gender\", \"Age_Gender_Race\",\n",
    "        \"Age_Race\", \"Gender_Race\",\n",
    "    ]\n",
    "    DIs = []\n",
    "    FVs = []\n",
    "    demog_trues = []\n",
    "    D2[\"Age_Gender\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int)) / 2\n",
    "    D2[\"Age_Race\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 2\n",
    "    D2[\"Gender_Race\"] = (D2[\"Gender_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 2\n",
    "    D2[\"Age_Gender_Race\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # laplace smoothing to account for zeros\n",
    "    for dc in demogColumns:\n",
    "      \n",
    "      positive_predictions = D2[\"probs_binarized\"]==1\n",
    "      positive_gold = D2[\"label_binarized\"]==1\n",
    "      protected = D2[dc]==0\n",
    "      privileged = D2[dc] == 1\n",
    "      N = 2\n",
    "      alpha = 1\n",
    "\n",
    "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
    "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
    "\n",
    "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
    "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
    "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
    "\n",
    "      try:\n",
    "        DI = DI_numerator / DI_denominator\n",
    "        ytrue = ytrue_numerator / ytrue_denominator\n",
    "        FV = np.abs(DI_numerator - ypred_global)\n",
    "      except:\n",
    "        DI=0\n",
    "        ytrue=0\n",
    "        FV=0\n",
    "      \n",
    "      if gold_ratio: \n",
    "          DI = DI / ytrue\n",
    "      #print(dc)\n",
    "\n",
    "      \n",
    "      DIs.append(DI) \n",
    "      FVs.append(FV)\n",
    "      demog_trues.append((dc, ytrue))\n",
    "\n",
    "    # auc from sklearn\n",
    "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # other metrics: MSE, pearson, F1\n",
    "    mse = 0 \n",
    "    pearsonscore, prob = 0, 0\n",
    "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
    "\n",
    "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
    "\n",
    "\n",
    "\n",
    "def get_results_HS(full_N, gold_ratio, models):\n",
    "    DIs, aucs, xaucs = [], [], []\n",
    "    task = []\n",
    "    dc_tracker = []\n",
    "    l = []\n",
    "    results = []\n",
    "    demog_trues = []\n",
    "\n",
    "    for basefname in models:\n",
    "        outs, dt = generatePlots_HS(basefname, full_N, gold_ratio)\n",
    "        if len(demog_trues) == 0:\n",
    "          demog_trues.extend(dt)\n",
    "        modelname = basefname.split(\"/\")[1]\n",
    "        results.append([modelname, m] + outs)\n",
    "\n",
    "    colnames = [\"model\", \n",
    "              \"DV\", \n",
    "              \"MSE\", \"Pearson R\", \"F1\",\n",
    "              \"AUC\", \n",
    "        \"DI_Age_bin\", \"DI_Gender_bin\", \"DI_Race_bin\",\n",
    "        \"DI_Age_Gender\", \"DI_Age_Gender_Race\",\n",
    "        \"DI_Age_Race\", \"DI_Gender_Race\",\n",
    "        \"FV_Age_bin\", \"FV_Gender_bin\", \"FV_Race_bin\",\n",
    "        \"FV_Age_Gender\", \"FV_Age_Gender_Race\",\n",
    "        \"FV_Age_Race\", \"FV_Gender_Race\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results) \n",
    "    df.columns = colnames\n",
    "\n",
    "    return df, demog_trues\n",
    "\n",
    "def calculate_fairness_HS(infile, outfile, weighted=False, unionYN=False):\n",
    "\n",
    "  models = [infile]\n",
    "\n",
    "  output, demogs = get_results_HS(\n",
    "    unionYN,\n",
    "    weighted, \n",
    "    models\n",
    "  )\n",
    "\n",
    "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
    "  wordlists = wordlists.split(\".\")[0]\n",
    "  output[\"model\"] = outfile\n",
    "  output[\"adjustedDI\"] = weighted\n",
    "  output[\"debiasing\"] = debiasing\n",
    "  output[\"wordlists\"] = wordlists\n",
    "  output[\"fullN\"] = unionYN\n",
    "\n",
    "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
    "  demog_df[\"model\"] = outfile\n",
    "  demog_df[\"ratio\"] = weighted\n",
    "\n",
    "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coOK4pEmCQ-L",
    "outputId": "320f8528-2cdc-4635-c44a-be527ffcd967"
   },
   "outputs": [],
   "source": [
    "\n",
    "taskNames = [\n",
    "            'Hatespeech_Hatespeech'\n",
    "]\n",
    "\n",
    "debiasing = [\n",
    "    \"PT\",\n",
    "    \"PTD\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "#    \"Continuous\",\n",
    "    \"Binary\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    \"BERT\",\n",
    "    \"RoBERTa\",\n",
    "    \"CNN\"\n",
    "]\n",
    "\n",
    "\n",
    "for m in taskNames:\n",
    "    for d in debiasing:\n",
    "        for t in tasks:\n",
    "          for mm in models:\n",
    "            fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
    "            outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
    "            calculate_fairness_HS(fname, outname, weighted=False, unionYN=False)\n",
    "            outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
    "            calculate_fairness_HS(fname, outname, weighted=True, unionYN=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eoM0e8mpbz6G"
   },
   "source": [
    "# MBTI Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cLFkZOm4by-V"
   },
   "outputs": [],
   "source": [
    "def generatePlots_MBTI(basefile, full_N=False, gold_ratio=False):\n",
    "    frames = []\n",
    "\n",
    "    fname = basefile\n",
    "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
    "    df = bert_preds\n",
    "\n",
    "    df[\"s1\"] = df[\"sentence\"]\n",
    "    D = df\n",
    "    if \"CNN\" in fname:\n",
    "      bertfile = fname.replace(\"CNN\", \"BERT\")\n",
    "      test_data = pd.read_csv(bertfile, quotechar=\"\\\"\", encoding=\"utf-8\")\n",
    "      D = df.merge(test_data, on=\"sentence\", how=\"inner\")\n",
    "      D[\"label\"] = D[\"label_y\"]\n",
    "      #print(\"start: \" + str(len(df[\"s1\"])))\n",
    "      #print(\"merge: \" + str(len(D[\"s1\"])))\n",
    "    else:\n",
    "      try:\n",
    "        D[\"probs\"] = D[\"preds\"]\n",
    "      except:\n",
    "        D[\"probs\"] = D[\"pred\"]\n",
    "\n",
    "    # For continuous, we'll use the stated label\n",
    "    # calculate median and use that as cutoff instead of 0.5 \n",
    "    median_val = 0.5\n",
    "    try:\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if float(x[1:-1]) < median_val else 1)\n",
    "    except:\n",
    "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if float(x) < median_val else 1) \n",
    "\n",
    "    #D[\"probs\"] = D[\"probs\"].apply(lambda x: float(x[1:-1]))\n",
    "    #D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
    "    D[\"label_binarized\"] = D[\"label\"]\n",
    "    #print(median_val)\n",
    "    D2 = D.dropna()\n",
    "\n",
    "    demogColumns = [\n",
    "        \"Age_bin\", \"Gender_bin\", \n",
    "        \"Age_Gender\"\n",
    "    ]\n",
    "    DIs = []\n",
    "    FVs = []\n",
    "    demog_trues = []\n",
    "    D2[\"Gender_bin\"] = (D2[\"x1\"] == \"m\").astype(int)\n",
    "    D2[\"Age_bin\"] = (D2[\"x2\"] < 55).astype(int)\n",
    "    D2[\"Age_Gender\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int)) / 2\n",
    "\n",
    "    # laplace smoothing to account for zeros\n",
    "    for dc in demogColumns:\n",
    "      \n",
    "      positive_predictions = D2[\"probs_binarized\"]==1\n",
    "      positive_gold = D2[\"label_binarized\"]==1\n",
    "      protected = D2[dc]==0\n",
    "      privileged = D2[dc] == 1\n",
    "      N = 2\n",
    "      alpha = 1\n",
    "\n",
    "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
    "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
    "\n",
    "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
    "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
    "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
    "\n",
    "      try:\n",
    "        DI = DI_numerator / DI_denominator\n",
    "        ytrue = ytrue_numerator / ytrue_denominator\n",
    "        FV = np.abs(DI_numerator - ypred_global)\n",
    "      except:\n",
    "        DI=0\n",
    "        ytrue=0\n",
    "        FV=0\n",
    "      \n",
    "      if gold_ratio: \n",
    "          DI = DI / ytrue\n",
    "      #print(dc)\n",
    "\n",
    "      \n",
    "      DIs.append(DI) \n",
    "      FVs.append(FV)\n",
    "      demog_trues.append((dc, ytrue))\n",
    "\n",
    "    # auc from sklearn\n",
    "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # other metrics: MSE, pearson, F1\n",
    "    mse = 0 \n",
    "    pearsonscore, prob = 0, 0\n",
    "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
    "\n",
    "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
    "\n",
    "\n",
    "\n",
    "def get_results_MBTI(full_N, gold_ratio, models):\n",
    "    DIs, aucs, xaucs = [], [], []\n",
    "    task = []\n",
    "    dc_tracker = []\n",
    "    l = []\n",
    "    results = []\n",
    "    demog_trues = []\n",
    "\n",
    "    for basefname in models:\n",
    "        outs, dt = generatePlots_MBTI(basefname, full_N, gold_ratio)\n",
    "        if len(demog_trues) == 0:\n",
    "          demog_trues.extend(dt)\n",
    "        modelname = basefname.split(\"/\")[1]\n",
    "        results.append([modelname, m] + outs)\n",
    "\n",
    "    colnames = [\"model\", \n",
    "              \"DV\", \n",
    "              \"MSE\", \"Pearson R\", \"F1\",\n",
    "              \"AUC\", \n",
    "        \"DI_Age_bin\", \"DI_Gender_bin\", \n",
    "        \"DI_Age_Gender\", \n",
    "        \"FV_Age_bin\", \"FV_Gender_bin\",\n",
    "        \"FV_Age_Gender\", \n",
    "    ]\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(results) \n",
    "    df.columns = colnames\n",
    "\n",
    "    return df, demog_trues\n",
    "\n",
    "def calculate_fairness_MBTI(infile, outfile, weighted=False, unionYN=False):\n",
    "\n",
    "  models = [infile]\n",
    "\n",
    "  output, demogs = get_results_MBTI(\n",
    "    unionYN,\n",
    "    weighted, \n",
    "    models\n",
    "  )\n",
    "\n",
    "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
    "  wordlists = wordlists.split(\".\")[0]\n",
    "  output[\"model\"] = outfile\n",
    "  output[\"adjustedDI\"] = weighted\n",
    "  output[\"debiasing\"] = debiasing\n",
    "  output[\"wordlists\"] = wordlists\n",
    "  output[\"fullN\"] = unionYN\n",
    "\n",
    "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
    "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
    "  demog_df[\"model\"] = outfile\n",
    "  demog_df[\"ratio\"] = weighted\n",
    "\n",
    "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzpfZKsLeDzq",
    "outputId": "59e12f80-ff92-4145-abbf-0cf6ece198c3"
   },
   "outputs": [],
   "source": [
    "taskNames = [\n",
    "            'MBTI_perceiving',\n",
    "             'MBTI_thinking',\n",
    "]\n",
    "\n",
    "debiasing = [\n",
    "    \"PT\",\n",
    "    \"PTD\"\n",
    "]\n",
    "\n",
    "tasks = [\n",
    "#    \"Continuous\",\n",
    "    \"Binary\"\n",
    "]\n",
    "\n",
    "models = [\n",
    "    \"BERT\",\n",
    "    \"RoBERTa\",\n",
    "#    \"CNN\"\n",
    "]\n",
    "\n",
    "\n",
    "for m in taskNames:\n",
    "    for d in debiasing:\n",
    "        for t in tasks:\n",
    "          for mm in models:\n",
    "            try:\n",
    "                fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
    "                outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
    "                calculate_fairness_MBTI(fname, outname, weighted=False, unionYN=False)\n",
    "                outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
    "                calculate_fairness_MBTI(fname, outname, weighted=True, unionYN=False)\n",
    "            except:\n",
    "                print(\"error\", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 fairness_output_Psychometric_TrustPhys_Continuous_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_Psych_FIPI.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j5aM0or61JgI"
   },
   "outputs": [],
   "source": [
    "# concatenate everything together\n",
    "!head -n 1 fairness_output_Hatespeech_Hatespeech_Binary_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_HS.csv\n",
    "!head -n 1 fairness_output_AskAPatient_AskAPatient_Continuous_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_AAP.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gq9NsWLuja1w"
   },
   "outputs": [],
   "source": [
    "!head -n 1 fairness_output_MBTI_perceiving_Binary_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_MBTI.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGMVaPgR2rLK",
    "outputId": "15c3541e-17bd-4732-c7ab-d9266c351f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> results_bert_acl22_AAP.csv <==\n",
      "model,DV,MSE,Pearson R,F1,AUC,DI_Age,DI_Gender,DI_Age_Gender,FV_Age,FV_Gender,FV_Age_Gender,adjustedDI,debiasing,wordlists,fullN\n",
      "\n",
      "==> results_bert_acl22_HS.csv <==\n",
      "model,DV,MSE,Pearson R,F1,AUC,DI_Age_bin,DI_Gender_bin,DI_Race_bin,DI_Age_Gender,DI_Age_Gender_Race,DI_Age_Race,DI_Gender_Race,FV_Age_bin,FV_Gender_bin,FV_Race_bin,FV_Age_Gender,FV_Age_Gender_Race,FV_Age_Race,FV_Gender_Race,adjustedDI,debiasing,wordlists,fullN\n",
      "\n",
      "==> results_bert_acl22_MBTI.csv <==\n",
      "model,DV,MSE,Pearson R,F1,AUC,DI_Age_bin,DI_Gender_bin,DI_Age_Gender,FV_Age_bin,FV_Gender_bin,FV_Age_Gender,adjustedDI,debiasing,wordlists,fullN\n",
      "\n",
      "==> results_bert_acl22_Psych_FIPI.csv <==\n",
      "model,DV,MSE,Pearson R,F1,AUC,DI_Age_bin,DI_Gender_bin,DI_Race_bin,DI_Education_bin,DI_Income_bin,DI_Age_Gender,DI_Age_Gender_Race,DI_Age_Gender_Race_Education,DI_Age_Gender_Race_Income,DI_Age_Gender_Education,DI_Age_Gender_Education_Income,DI_Age_Gender_Income,DI_Age_Race,DI_Age_Race_Education,DI_Age_Race_Education_Income,DI_Age_Race_Income,DI_Age_Education,DI_Age_Education_Income,DI_Age_Income,DI_Gender_Race,DI_Gender_Race_Education,DI_Gender_Race_Education_Income,DI_Gender_Race_Income,DI_Gender_Education,DI_Gender_Education_Income,DI_Gender_Income,DI_Race_Education,DI_Race_Education_Income,DI_Race_Income,DI_Education_Income,DI_Age_Gender_Race_Education_Income,FV_Age_bin,FV_Gender_bin,FV_Race_bin,FV_Education_bin,FV_Income_bin,FV_Age_Gender,FV_Age_Gender_Race,FV_Age_Gender_Race_Education,FV_Age_Gender_Race_Income,FV_Age_Gender_Education,FV_Age_Gender_Education_Income,FV_Age_Gender_Income,FV_Age_Race,FV_Age_Race_Education,FV_Age_Race_Education_Income,FV_Age_Race_Income,FV_Age_Education,FV_Age_Education_Income,FV_Age_Income,FV_Gender_Race,FV_Gender_Race_Education,FV_Gender_Race_Education_Income,FV_Gender_Race_Income,FV_Gender_Education,FV_Gender_Education_Income,FV_Gender_Income,FV_Race_Education,FV_Race_Education_Income,FV_Race_Income,FV_Education_Income,FV_Age_Gender_Race_Education_Income,adjustedDI,debiasing,wordlists,fullN\n"
     ]
    }
   ],
   "source": [
    "!head results_bert_acl22*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4Y57gD072tgH"
   },
   "outputs": [],
   "source": [
    "!tail -n 1 -q fairness_output_Psych* >> results_bert_acl22_Psych_FIPI.csv\n",
    "!tail -n 1 -q fairness_output_Ha* >> results_bert_acl22_HS.csv\n",
    "!tail -n 1 -q fairness_output_Ask* >> results_bert_acl22_AAP.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QwsLk4I8jfo0"
   },
   "outputs": [],
   "source": [
    "!tail -n 1 -q fairness_output_MBTI* >> results_bert_acl22_MBTI.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ACL2022_fairness_metrics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
