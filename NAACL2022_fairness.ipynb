{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtgbrM76oD9s"
      },
      "source": [
        "# ACL 2022 Submission: Fairness Calculations\n",
        "\n",
        "The input required here is the merged prediction files, and the gold standard data with the relevant demographic information.\n",
        "\n",
        "Output is a spreadsheet with calculated intersectional DI scores.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First the following upgrades are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75mLLOyW_lBE",
        "outputId": "8e7ad4be-61a8-4e88-bab8-465b34ce3473"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6IMa8vE5aW"
      },
      "source": [
        "## Restart runtime after executing the above cell to get the latest version of pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-meESHofN0uc"
      },
      "source": [
        "# Psychometric and FIPI Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljhNFRHEytvn",
        "outputId": "74e299cb-6ae3-46ea-ab4b-92d4aeaf7b87"
      },
      "outputs": [],
      "source": [
        "# load libraries and data\n",
        "# to replicate on google colab just drag and drop relevant files into the directory (they do not persist)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import statistics\n",
        "\n",
        "from aif360.sklearn.metrics import disparate_impact_ratio\n",
        "from aif360.datasets import StandardDataset\n",
        "\n",
        "# need this for psychometric data, the other ones have demographics in the files\n",
        "test_data_psych = pd.read_excel(\"PsychometricData.xlsx\", skiprows=lambda x: x in [1], header=0)\n",
        "test_data_fipi = pd.read_csv(\"fipi.csv\")\n",
        "HS_train = pd.read_csv(\"trainHS.csv\")\n",
        "HS_valid = pd.read_csv(\"validHS.csv\")\n",
        "HS_test = pd.read_csv(\"testHS.csv\")\n",
        "frames = [HS_train, HS_valid, HS_test]\n",
        "test_data_HS = pd.concat(frames)\n",
        "\n",
        "test_data_psych = test_data_psych[[\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\n",
        "            \"Text_TrustPhys\",\"Label_SubjectiveLit\",\"Label_TrustPhys\",\n",
        "            \"Label_Anxiety\",\"Label_Numeracy\",\"D1\",\n",
        "            \"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\n",
        "            ]]\n",
        "\n",
        "test_data_fipi = test_data_fipi[[\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\n",
        "            \"Text_TrustPhys\",\"Label_SubjectiveLit\",\"Label_TrustPhys\",\n",
        "            \"Label_Anxiety\",\"Label_Numeracy\",\"D1\",\n",
        "            \"D2\",\"D3\",\"D4\",\"D5\",\"D6\",\n",
        "            ]]\n",
        "\n",
        "test_data_HS = test_data_HS[[\"text\", \"gender\", \"age\",\"country\",\"ethnicity\",\"label\"]]\n",
        "\n",
        "\n",
        "test_data_fipi[['D1', 'D2', 'D3', 'D4', 'D5']] = test_data_fipi[['D1', 'D2', 'D3', 'D4', 'D5']].apply(pd.to_numeric, errors='coerce')\n",
        "test_data_psych[['D1', 'D2', 'D3', 'D4', 'D5']] = test_data_psych[['D1', 'D2', 'D3', 'D4', 'D5']].apply(pd.to_numeric, errors='coerce')\n",
        "test_data_HS[[\"gender\", \"age\",\"ethnicity\",\"label\"]] = test_data_HS[[\"gender\", \"age\",\"ethnicity\",\"label\"]].apply(pd.to_numeric, errors='coerce')\n",
        "print(test_data_HS.gender)\n",
        "test_data_psych.dropna(subset=['D1', 'D2', 'D3', 'D4', 'D5'], inplace=True)\n",
        "test_data_fipi.dropna(subset=['D1', 'D2', 'D3', 'D4', 'D5'], inplace=True)\n",
        "test_data_HS.dropna(subset=[\"gender\", \"age\",\"ethnicity\",\"label\"], inplace=True)\n",
        "print(test_data_HS.gender)\n",
        "\n",
        "test_data_HS.rename(\n",
        "    columns={\n",
        "        \"gender\":\"Gender_bin\",\n",
        "        \"age\":\"Age_bin\",\n",
        "        \"ethnicity\":\"Race_bin\"\n",
        "    },\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "test_data_HS = test_data_HS[~test_data_HS.text.str.contains(\"user user user\")]\n",
        "print(len(test_data_HS.text))\n",
        "\"\"\"\n",
        "### Demographic binarization assumptions\n",
        "\n",
        "- D1 (Age): Over/under 55\n",
        "- D2 (Gender): already binarized\n",
        "- D3 (Race): White/non-White\n",
        "- D4 (Education): College grad or higher yes/no\n",
        "- D5 (Income): $55k+ yes/no \n",
        "\"\"\"\n",
        "\n",
        "# first binarize all of our columns.\n",
        "test_data_fipi[\"Age_bin\"] = (test_data_fipi[\"D1\"] <= 38).astype(int)\n",
        "test_data_fipi[\"Gender_bin\"] = (test_data_fipi[\"D2\"] == 1).astype(int)\n",
        "test_data_fipi[\"Race_bin\"] = (test_data_fipi[\"D3\"] == 1).astype(int)\n",
        "test_data_fipi[\"Education_bin\"] = (test_data_fipi[\"D4\"] >= 5).astype(int)\n",
        "test_data_fipi[\"Income_bin\"] = (test_data_fipi[\"D5\"] >= 4).astype(int)\n",
        "\n",
        "test_data_psych[\"Age_bin\"] = (test_data_psych[\"D1\"] <= 38).astype(int)\n",
        "test_data_psych[\"Gender_bin\"] = (test_data_psych[\"D2\"] == 1).astype(int)\n",
        "test_data_psych[\"Race_bin\"] = (test_data_psych[\"D3\"] == 1).astype(int)\n",
        "test_data_psych[\"Education_bin\"] = (test_data_psych[\"D4\"] >= 5).astype(int)\n",
        "test_data_psych[\"Income_bin\"] = (test_data_psych[\"D5\"] >= 4).astype(int)\n",
        "\n",
        "# I should be able to calculate all intersections programmatically. \n",
        "\n",
        "column_names = [\"Age_bin\", \"Gender_bin\", \"Race_bin\", \"Education_bin\", \"Income_bin\"]\n",
        "\n",
        "combinations = []\n",
        "# two-way \n",
        "for aa in range(len(column_names)):\n",
        "  a = column_names[aa]\n",
        "  for bb in range(aa + 1, len(column_names)):\n",
        "    b = column_names[bb]\n",
        "    if a == b:\n",
        "      continue \n",
        "    combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0])\n",
        "    for cc in range(bb + 1, len(column_names)):\n",
        "      c = column_names[cc]\n",
        "      if a == b or a == c or b == c:\n",
        "        continue \n",
        "      combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0] + \"_\" + c.split(\"_\")[0])\n",
        "      for dd in range(cc + 1, len(column_names)):\n",
        "        d = column_names[dd]  \n",
        "        if a == b or a == c or a == d or b == c or b == d or c == d:\n",
        "          continue \n",
        "        combinations.append(a.split(\"_\")[0] + \"_\" + b.split(\"_\")[0] + \"_\" + c.split(\"_\")[0] + \"_\" + d.split(\"_\")[0])\n",
        "      \n",
        "combinations.append(\"Age_Gender_Race_Education_Income\")\n",
        "\n",
        "print(combinations)\n",
        "\n",
        "# this will be a fraction: what percentage of categories is someone in the privileged class? \n",
        "\n",
        "\n",
        "for comb in combinations:\n",
        "  columns = [a + \"_bin\" for a in comb.split(\"_\")]\n",
        "  test_data_psych[comb] = 0\n",
        "  for i in range(len(columns)):\n",
        "    test_data_psych[comb] += test_data_psych[columns[i]].astype(float)\n",
        "  test_data_psych[comb] = test_data_psych[comb] / len(columns) \n",
        "\n",
        "  test_data_fipi[comb] = 0\n",
        "  for i in range(len(columns)):\n",
        "    test_data_fipi[comb] += test_data_fipi[columns[i]].astype(float)\n",
        "  test_data_fipi[comb] = test_data_fipi[comb] / len(columns) \n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJYava8MjS-W"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error, f1_score\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SVjW-J_qPH8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generatePlots_v3(basefile, modelTask, textCol, full_N=False, gold_ratio=False):\n",
        "    frames = []\n",
        "\n",
        "    fname = basefile\n",
        "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
        "    df = bert_preds\n",
        "\n",
        "    df[\"s1\"] = df[\"sentence\"]\n",
        "    if \"FIPI\" in basefile:\n",
        "      test_data = test_data_fipi\n",
        "    else:\n",
        "      test_data = test_data_psych\n",
        "    test_data[\"s1\"] = test_data[textCol]\n",
        "    D = df.merge(test_data, on=\"s1\", how=\"inner\")\n",
        "    print(\"start: \" + str(len(df[\"s1\"])))\n",
        "    print(\"merge: \" + str(len(D[\"s1\"])))\n",
        "\n",
        "    try:\n",
        "      D[\"probs\"] = D[\"preds\"]\n",
        "    except:\n",
        "      D[\"probs\"] = D[\"pred\"]\n",
        "\n",
        "    # For continuous, we'll use the stated label\n",
        "    # calculate median and use that as cutoff instead of 0.5 \n",
        "    if \"Continuous\" in basefile:\n",
        "      labelColumn = textCol.replace(\"Text\", \"Label\")\n",
        "      median_val = statistics.median(D[labelColumn])\n",
        "      try:\n",
        "        print(D[\"label\"])\n",
        "      except:\n",
        "        D[\"label\"] = D[labelColumn]\n",
        "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
        "      D[\"label_binarized\"] = D[\"label\"].apply(lambda x: 0 if x < median_val else 1)\n",
        "      print(median_val)\n",
        "    else:\n",
        "      D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < 0.5 else 1)\n",
        "      D[\"label_binarized\"] = D[\"label\"]\n",
        "\n",
        "    D2 = D\n",
        "\n",
        "    demogColumns = [\n",
        "                    \"Age_bin\", \"Gender_bin\", \"Race_bin\",\n",
        "        \"Education_bin\", \"Income_bin\", \"Age_Gender\", \"Age_Gender_Race\",\n",
        "        \"Age_Gender_Race_Education\", \"Age_Gender_Race_Income\",\n",
        "        \"Age_Gender_Education\", \"Age_Gender_Education_Income\",\n",
        "        \"Age_Gender_Income\", \"Age_Race\", \"Age_Race_Education\",\n",
        "        \"Age_Race_Education_Income\", \"Age_Race_Income\", \"Age_Education\",\n",
        "        \"Age_Education_Income\", \"Age_Income\", \"Gender_Race\",\n",
        "        \"Gender_Race_Education\", \"Gender_Race_Education_Income\",\n",
        "        \"Gender_Race_Income\", \"Gender_Education\", \"Gender_Education_Income\",\n",
        "        \"Gender_Income\", \"Race_Education\", \"Race_Education_Income\",\n",
        "        \"Race_Income\", \"Education_Income\", \"Age_Gender_Race_Education_Income\"\n",
        "    ]\n",
        "    DIs = []\n",
        "    demog_trues = []\n",
        "    FVs = []\n",
        "\n",
        "    # laplace smoothing to account for zeros\n",
        "    for dc in demogColumns:\n",
        "      \n",
        "      positive_predictions = D2[\"probs_binarized\"]==1\n",
        "      positive_gold = D2[\"label_binarized\"]==1\n",
        "      protected = D2[dc]==0\n",
        "      privileged = D2[dc] == 1\n",
        "      N = 2\n",
        "      alpha = 1\n",
        "\n",
        "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
        "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
        "\n",
        "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
        "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
        "\n",
        "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
        "\n",
        "\n",
        "      try:\n",
        "        DI = DI_numerator / DI_denominator\n",
        "        ytrue = ytrue_numerator / ytrue_denominator\n",
        "        FV = np.abs(DI_numerator - ypred_global)\n",
        "      except:\n",
        "        DI=0\n",
        "        ytrue=0\n",
        "        FV=0\n",
        "      \n",
        "      if gold_ratio: \n",
        "          DI = DI / ytrue\n",
        "      \n",
        "      DIs.append(DI) \n",
        "      FVs.append(FV)\n",
        "      demog_trues.append((dc, ytrue))\n",
        "\n",
        "    # auc from sklearn\n",
        "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # other metrics: MSE, pearson, F1\n",
        "    mse = mean_squared_error(D2[\"label\"], D2[\"probs\"])\n",
        "    pearsonscore, prob = pearsonr(D2[\"label\"], D2[\"probs\"])\n",
        "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
        "\n",
        "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
        "\n",
        "\n",
        "\n",
        "def get_results(full_N, gold_ratio, models):\n",
        "    DIs, aucs, xaucs = [], [], []\n",
        "    task = []\n",
        "    dc_tracker = []\n",
        "    l = []\n",
        "    results = []\n",
        "    demog_trues = []\n",
        "\n",
        "    for basefname in models:\n",
        "      for i in range(len(modelTasks)):\n",
        "        m = modelTasks[i]\n",
        "        l.append(m) \n",
        "        print(i, textCols)\n",
        "        colname = textCols[i]\n",
        "        print(basefname, m, colname)\n",
        "        if m.lower() not in basefname.lower():\n",
        "          if 'FIPI' in basefname and m.lower() ==\"subjectivelit\":\n",
        "            m = \"SubjectiveLit\"\n",
        "          else:\n",
        "            continue\n",
        "        outs, dt = generatePlots_v3(basefname, m, colname, full_N, gold_ratio)\n",
        "        if len(demog_trues) == 0:\n",
        "          demog_trues.extend(dt)\n",
        "        modelname = basefname.split(\"/\")[1]\n",
        "        results.append([modelname, m] + outs)\n",
        "\n",
        "    colnames = [\"model\", \n",
        "              \"DV\", \n",
        "              \"MSE\", \"Pearson R\", \"F1\",\n",
        "              \"AUC\", \n",
        "        \"DI_Age_bin\", \"DI_Gender_bin\", \"DI_Race_bin\",\n",
        "        \"DI_Education_bin\", \"DI_Income_bin\", \"DI_Age_Gender\", \"DI_Age_Gender_Race\",\n",
        "        \"DI_Age_Gender_Race_Education\", \"DI_Age_Gender_Race_Income\",\n",
        "        \"DI_Age_Gender_Education\", \"DI_Age_Gender_Education_Income\",\n",
        "        \"DI_Age_Gender_Income\", \"DI_Age_Race\", \"DI_Age_Race_Education\",\n",
        "        \"DI_Age_Race_Education_Income\", \"DI_Age_Race_Income\", \"DI_Age_Education\",\n",
        "        \"DI_Age_Education_Income\", \"DI_Age_Income\", \"DI_Gender_Race\",\n",
        "        \"DI_Gender_Race_Education\", \"DI_Gender_Race_Education_Income\",\n",
        "        \"DI_Gender_Race_Income\", \"DI_Gender_Education\", \"DI_Gender_Education_Income\",\n",
        "        \"DI_Gender_Income\", \"DI_Race_Education\", \"DI_Race_Education_Income\",\n",
        "        \"DI_Race_Income\", \"DI_Education_Income\", \"DI_Age_Gender_Race_Education_Income\",\n",
        "        \"FV_Age_bin\", \"FV_Gender_bin\", \"FV_Race_bin\",\n",
        "        \"FV_Education_bin\", \"FV_Income_bin\", \"FV_Age_Gender\", \"FV_Age_Gender_Race\",\n",
        "        \"FV_Age_Gender_Race_Education\", \"FV_Age_Gender_Race_Income\",\n",
        "        \"FV_Age_Gender_Education\", \"FV_Age_Gender_Education_Income\",\n",
        "        \"FV_Age_Gender_Income\", \"FV_Age_Race\", \"FV_Age_Race_Education\",\n",
        "        \"FV_Age_Race_Education_Income\", \"FV_Age_Race_Income\", \"FV_Age_Education\",\n",
        "        \"FV_Age_Education_Income\", \"FV_Age_Income\", \"FV_Gender_Race\",\n",
        "        \"FV_Gender_Race_Education\", \"FV_Gender_Race_Education_Income\",\n",
        "        \"FV_Gender_Race_Income\", \"FV_Gender_Education\", \"FV_Gender_Education_Income\",\n",
        "        \"FV_Gender_Income\", \"FV_Race_Education\", \"FV_Race_Education_Income\",\n",
        "        \"FV_Race_Income\", \"FV_Education_Income\", \"FV_Age_Gender_Race_Education_Income\",\n",
        "    ]\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame(results) \n",
        "    df.columns = colnames\n",
        "\n",
        "    return df, demog_trues\n",
        "\n",
        "def calculate_fairness(infile, outfile, weighted=False, unionYN=False):\n",
        "\n",
        "  models = [infile]\n",
        "\n",
        "  output, demogs = get_results(\n",
        "    unionYN,\n",
        "    weighted, \n",
        "    models\n",
        "  )\n",
        "\n",
        "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
        "  wordlists = wordlists.split(\".\")[0]\n",
        "  output[\"model\"] = outfile\n",
        "  output[\"adjustedDI\"] = weighted\n",
        "  output[\"debiasing\"] = debiasing\n",
        "  output[\"wordlists\"] = wordlists\n",
        "  output[\"fullN\"] = unionYN\n",
        "\n",
        "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
        "  demog_df[\"model\"] = outfile\n",
        "  demog_df[\"ratio\"] = weighted\n",
        "\n",
        "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwUjw75w-N3X",
        "outputId": "bede23aa-5c2e-4b1e-89ac-d5ad169c3a5c"
      },
      "outputs": [],
      "source": [
        "modelTasks = [\"Anxiety\", \"Numeracy\", \"SubjectiveLit\", \"TrustPhys\"]\n",
        "\n",
        "textCols = [\"Text_Anxiety\",\"Text_Numeracy\",\"Text_SubjectiveLit\",\"Text_TrustPhys\"]\n",
        "\n",
        "labelCols = [\"Label_Anxiety\",\"Label_Numeracy\",\"Label_SubjectiveLit\",\"Label_TrustPhys\"]\n",
        "taskNames = [\n",
        "            'Psychometric_Anxiety',\n",
        "            'Psychometric_Numeracy',\n",
        "            'Psychometric_SubjectiveLit',\n",
        "            'Psychometric_TrustPhys',\n",
        "            'FIPI_Extraverted',\n",
        "            'FIPI_Stable'\n",
        "]\n",
        "\n",
        "debiasing = [\n",
        "    \"PT\",\n",
        "    \"PTD\",\n",
        "    \"PTDCDA\",\n",
        "    \"PTDDropout\"\n",
        "]\n",
        "\n",
        "tasks = [\n",
        "    \"Continuous\",\n",
        "    \"Binary\"\n",
        "]\n",
        "\n",
        "models = [\n",
        "    \"BERT\",\n",
        "    \"RoBERTa\",\n",
        "    \"CNN\"\n",
        "]\n",
        "\n",
        "\n",
        "for m in taskNames:\n",
        "    for d in debiasing:\n",
        "        for t in tasks:\n",
        "          for mm in models:\n",
        "              fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
        "              outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
        "              calculate_fairness(fname, outname, weighted=False, unionYN=False)\n",
        "              outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
        "              calculate_fairness(fname, outname, weighted=True, unionYN=False)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JrHNWrcN5FL"
      },
      "source": [
        "# Ask a Patient Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ0fVsybNV7V"
      },
      "outputs": [],
      "source": [
        "def generatePlots_AAP(basefile, full_N=False, gold_ratio=False):\n",
        "    frames = []\n",
        "\n",
        "    fname = basefile\n",
        "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
        "    D = bert_preds\n",
        "    df = D\n",
        "\n",
        "    if \"CNN\" in fname:\n",
        "      bertfile = fname.replace(\"CNN\", \"BERT\")\n",
        "      test_data = pd.read_csv(bertfile, quotechar=\"\\\"\", encoding=\"utf-8\")\n",
        "      D = df.merge(test_data, on=\"sentence\", how=\"inner\")\n",
        "      print(\"start: \" + str(len(df[\"sentence\"])))\n",
        "      print(\"merge: \" + str(len(D[\"sentence\"])))\n",
        "      try:\n",
        "        D[\"probs\"] = D[\"preds\"]\n",
        "      except:\n",
        "        D[\"probs\"] = D[\"pred\"]\n",
        "    else:\n",
        "      try:\n",
        "        D[\"probs\"] = D[\"preds\"]\n",
        "      except:\n",
        "        D[\"probs\"] = D[\"pred\"]\n",
        "\n",
        "    # For continuous, we'll use the stated label\n",
        "    # calculate median and use that as cutoff instead of 0.5 \n",
        "    #median_val = 0.5\n",
        "    median_val = statistics.median(D[\"label\"])\n",
        "\n",
        "    D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
        "    D[\"label_binarized\"] = D[\"label\"].apply(lambda x: 0 if x < median_val else 1)\n",
        "    print(median_val)\n",
        "    D2 = D.dropna()\n",
        "\n",
        "    demogColumns = [\n",
        "                    \"Age\", \"Gender\", \"Age_Gender\"\n",
        "    ]\n",
        "    DIs = []\n",
        "    FVs=[]\n",
        "    demog_trues = []\n",
        "    D2[\"Age\"] = (D2[\"x2\"] <= 56).astype(int)\n",
        "    D2[\"Gender\"] = (D2[\"x1\"] == \"M\").astype(int)\n",
        "    D2[\"Age_Gender\"] = (D2[\"Age\"].astype(int) + D2[\"Gender\"].astype(int)) / 2\n",
        "    \n",
        "\n",
        "    for dc in demogColumns:\n",
        "      \n",
        "      positive_predictions = D2[\"probs_binarized\"]==1\n",
        "      positive_gold = D2[\"label_binarized\"]==1\n",
        "      protected = D2[dc]==0\n",
        "      privileged = D2[dc] == 1\n",
        "      N = 2\n",
        "      alpha = 1\n",
        "      \n",
        "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
        "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
        "\n",
        "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
        "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
        "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
        "\n",
        "      DI = DI_numerator / DI_denominator\n",
        "      ytrue = ytrue_numerator / ytrue_denominator\n",
        "      FV = np.abs(DI_numerator - ypred_global)\n",
        "      \n",
        "      if gold_ratio: \n",
        "          DI = DI / ytrue\n",
        "      print(dc)\n",
        "\n",
        "      \n",
        "      DIs.append(DI) \n",
        "      FVs.append(FV)\n",
        "      demog_trues.append((dc, ytrue))\n",
        "\n",
        "    # auc from sklearn\n",
        "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "    mse = mean_squared_error(D2[\"label\"], D2[\"probs\"])\n",
        "    pearsonscore, prob = pearsonr(D2[\"label\"], D2[\"probs\"])\n",
        "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
        "\n",
        "    return [mse, pearsonscore, f1score, auc] + DIs+FVs, demog_trues\n",
        "\n",
        "\n",
        "def get_results_AAP(full_N, gold_ratio, models):\n",
        "    DIs, aucs, xaucs = [], [], []\n",
        "    task = []\n",
        "    dc_tracker = []\n",
        "    l = []\n",
        "    results = []\n",
        "    demog_trues = []\n",
        "\n",
        "    for basefname in models:\n",
        "        outs, dt = generatePlots_AAP(basefname, full_N, gold_ratio)\n",
        "        if len(demog_trues) == 0:\n",
        "          demog_trues.extend(dt)\n",
        "        modelname = basefname.split(\"/\")[1]\n",
        "        results.append([modelname, m] + outs)\n",
        "\n",
        "    colnames = [\"model\", \n",
        "              \"DV\", \n",
        "              \"MSE\", \"Pearson R\", \"F1\",\n",
        "              \"AUC\", \n",
        "        \"DI_Age\", \"DI_Gender\", \"DI_Age_Gender\",\n",
        "        \"FV_Age\", \"FV_Gender\", \"FV_Age_Gender\", \n",
        "    ]\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame(results) \n",
        "    df.columns = colnames\n",
        "\n",
        "    return df, demog_trues\n",
        "\n",
        "def calculate_fairness_AAP(infile, outfile, weighted=False, unionYN=False):\n",
        "\n",
        "  models = [infile]\n",
        "\n",
        "  output, demogs = get_results_AAP(\n",
        "    unionYN,\n",
        "    weighted, \n",
        "    models\n",
        "  )\n",
        "\n",
        "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
        "  wordlists = wordlists.split(\".\")[0]\n",
        "  output[\"model\"] = outfile\n",
        "  output[\"adjustedDI\"] = weighted\n",
        "  output[\"debiasing\"] = debiasing\n",
        "  output[\"wordlists\"] = wordlists\n",
        "  output[\"fullN\"] = unionYN\n",
        "\n",
        "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
        "  demog_df[\"model\"] = outfile\n",
        "  demog_df[\"ratio\"] = weighted\n",
        "\n",
        "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXjevffENJyn",
        "outputId": "a06b2b36-8c1c-4d4a-8a68-f307ff627a7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "taskNames = [\n",
        "            'AskAPatient_AskAPatient'\n",
        "]\n",
        "\n",
        "debiasing = [\n",
        "    \"PT\",\n",
        "    \"PTD\"\n",
        "]\n",
        "\n",
        "tasks = [\n",
        "    \"Continuous\",\n",
        "    \"Binary\"\n",
        "]\n",
        "\n",
        "models = [\n",
        "    \"BERT\",\n",
        "    \"RoBERTa\"\n",
        "    \"CNN\"\n",
        "]\n",
        "\n",
        "\n",
        "for m in taskNames:\n",
        "    for d in debiasing:\n",
        "        for t in tasks:\n",
        "          for mm in models:\n",
        "            fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
        "            calculate_fairness_AAP(fname, outname, weighted=False, unionYN=False)\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
        "            calculate_fairness_AAP(fname, outname, weighted=True, unionYN=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8CUDxovCLC7"
      },
      "source": [
        "# Hate Speech Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl6Mo4pKCMbM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generatePlots_HS(basefile, full_N=False, gold_ratio=False):\n",
        "    frames = []\n",
        "\n",
        "    fname = basefile\n",
        "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
        "    df = bert_preds\n",
        "\n",
        "    df[\"s1\"] = df[\"sentence\"]\n",
        "    test_data = test_data_HS\n",
        "    test_data[\"s1\"] = test_data[\"text\"]\n",
        "    D = df.merge(test_data, on=\"s1\", how=\"inner\")\n",
        "    D.drop_duplicates(subset=[\"s1\"], inplace=True)\n",
        "    D[\"label\"] = D[\"label_y\"]\n",
        "    print(\"start: \" + str(len(df[\"s1\"])))\n",
        "    print(\"merge: \" + str(len(D[\"s1\"])))\n",
        "\n",
        "    try:\n",
        "      D[\"probs\"] = D[\"preds\"]\n",
        "    except:\n",
        "      try:\n",
        "        D[\"probs\"] = D[\"pred\"]\n",
        "      except:\n",
        "        D[\"probs\"] = D[\"probs\"]\n",
        "\n",
        "    # For continuous, we'll use the stated label\n",
        "    # calculate median and use that as cutoff instead of 0.5 \n",
        "    median_val = 0.5\n",
        "    D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if float(x[1:-1]) < median_val else 1)\n",
        "    D[\"probs\"] = D[\"probs_binarized\"]\n",
        "    D[\"label_binarized\"] = D[\"label\"]\n",
        "    print(median_val)\n",
        "    D2 = D.dropna()\n",
        "\n",
        "    demogColumns = [\n",
        "                    \"Age_bin\", \"Gender_bin\", \"Race_bin\",\n",
        "        \"Age_Gender\", \"Age_Gender_Race\",\n",
        "        \"Age_Race\", \"Gender_Race\",\n",
        "    ]\n",
        "    DIs = []\n",
        "    FVs = []\n",
        "    demog_trues = []\n",
        "    D2[\"Age_Gender\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int)) / 2\n",
        "    D2[\"Age_Race\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 2\n",
        "    D2[\"Gender_Race\"] = (D2[\"Gender_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 2\n",
        "    D2[\"Age_Gender_Race\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int) + D2[\"Race_bin\"].astype(int)) / 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # laplace smoothing to account for zeros\n",
        "    for dc in demogColumns:\n",
        "      \n",
        "      positive_predictions = D2[\"probs_binarized\"]==1\n",
        "      positive_gold = D2[\"label_binarized\"]==1\n",
        "      protected = D2[dc]==0\n",
        "      privileged = D2[dc] == 1\n",
        "      N = 2\n",
        "      alpha = 1\n",
        "\n",
        "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
        "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
        "\n",
        "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
        "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
        "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
        "\n",
        "      try:\n",
        "        DI = DI_numerator / DI_denominator\n",
        "        ytrue = ytrue_numerator / ytrue_denominator\n",
        "        FV = np.abs(DI_numerator - ypred_global)\n",
        "      except:\n",
        "        DI=0\n",
        "        ytrue=0\n",
        "        FV=0\n",
        "      \n",
        "      if gold_ratio: \n",
        "          DI = DI / ytrue\n",
        "      print(dc)\n",
        "\n",
        "      \n",
        "      DIs.append(DI) \n",
        "      FVs.append(FV)\n",
        "      demog_trues.append((dc, ytrue))\n",
        "\n",
        "    # auc from sklearn\n",
        "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # other metrics: MSE, pearson, F1\n",
        "    mse = 0 \n",
        "    pearsonscore, prob = 0, 0\n",
        "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
        "\n",
        "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
        "\n",
        "\n",
        "\n",
        "def get_results_HS(full_N, gold_ratio, models):\n",
        "    DIs, aucs, xaucs = [], [], []\n",
        "    task = []\n",
        "    dc_tracker = []\n",
        "    l = []\n",
        "    results = []\n",
        "    demog_trues = []\n",
        "\n",
        "    for basefname in models:\n",
        "        outs, dt = generatePlots_HS(basefname, full_N, gold_ratio)\n",
        "        if len(demog_trues) == 0:\n",
        "          demog_trues.extend(dt)\n",
        "        modelname = basefname.split(\"/\")[1]\n",
        "        results.append([modelname, m] + outs)\n",
        "\n",
        "    colnames = [\"model\", \n",
        "              \"DV\", \n",
        "              \"MSE\", \"Pearson R\", \"F1\",\n",
        "              \"AUC\", \n",
        "        \"DI_Age_bin\", \"DI_Gender_bin\", \"DI_Race_bin\",\n",
        "        \"DI_Age_Gender\", \"DI_Age_Gender_Race\",\n",
        "        \"DI_Age_Race\", \"DI_Gender_Race\",\n",
        "        \"FV_Age_bin\", \"FV_Gender_bin\", \"FV_Race_bin\",\n",
        "        \"FV_Age_Gender\", \"FV_Age_Gender_Race\",\n",
        "        \"FV_Age_Race\", \"FV_Gender_Race\",\n",
        "    ]\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame(results) \n",
        "    df.columns = colnames\n",
        "\n",
        "    return df, demog_trues\n",
        "\n",
        "def calculate_fairness_HS(infile, outfile, weighted=False, unionYN=False):\n",
        "\n",
        "  models = [infile]\n",
        "\n",
        "  output, demogs = get_results_HS(\n",
        "    unionYN,\n",
        "    weighted, \n",
        "    models\n",
        "  )\n",
        "\n",
        "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
        "  wordlists = wordlists.split(\".\")[0]\n",
        "  output[\"model\"] = outfile\n",
        "  output[\"adjustedDI\"] = weighted\n",
        "  output[\"debiasing\"] = debiasing\n",
        "  output[\"wordlists\"] = wordlists\n",
        "  output[\"fullN\"] = unionYN\n",
        "\n",
        "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
        "  demog_df[\"model\"] = outfile\n",
        "  demog_df[\"ratio\"] = weighted\n",
        "\n",
        "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coOK4pEmCQ-L",
        "outputId": "320f8528-2cdc-4635-c44a-be527ffcd967"
      },
      "outputs": [],
      "source": [
        "\n",
        "taskNames = [\n",
        "            'HateSpeech_HateSpeech'\n",
        "]\n",
        "\n",
        "debiasing = [\n",
        "    \"PT\",\n",
        "    \"PTD\"\n",
        "]\n",
        "\n",
        "tasks = [\n",
        "    \"Continuous\",\n",
        "    \"Binary\"\n",
        "]\n",
        "\n",
        "models = [\n",
        "    \"BERT\",\n",
        "    \"RoBERTa\"\n",
        "    \"CNN\"\n",
        "]\n",
        "\n",
        "\n",
        "for m in taskNames:\n",
        "    for d in debiasing:\n",
        "        for t in tasks:\n",
        "          for mm in models:\n",
        "            fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
        "            calculate_fairness_HS(fname, outname, weighted=False, unionYN=False)\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
        "            calculate_fairness_HS(fname, outname, weighted=True, unionYN=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoM0e8mpbz6G"
      },
      "source": [
        "# MBTI Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLFkZOm4by-V"
      },
      "outputs": [],
      "source": [
        "def generatePlots_MBTI(basefile, full_N=False, gold_ratio=False):\n",
        "    frames = []\n",
        "\n",
        "    fname = basefile\n",
        "    bert_preds = pd.read_csv(fname, quotechar=\"\\\"\", encoding=\"utf-8\") \n",
        "    df = bert_preds\n",
        "\n",
        "    df[\"s1\"] = df[\"sentence\"]\n",
        "    D = df\n",
        "    if \"CNN\" in fname:\n",
        "      bertfile = fname.replace(\"CNN\", \"BERT\")\n",
        "      test_data = pd.read_csv(bertfile, quotechar=\"\\\"\", encoding=\"utf-8\")\n",
        "      D = df.merge(test_data, on=\"sentence\", how=\"inner\")\n",
        "      D[\"label\"] = D[\"label_y\"]\n",
        "      print(\"start: \" + str(len(df[\"s1\"])))\n",
        "      print(\"merge: \" + str(len(D[\"s1\"])))\n",
        "    else:\n",
        "      try:\n",
        "        D[\"probs\"] = D[\"preds\"]\n",
        "      except:\n",
        "        D[\"probs\"] = D[\"pred\"]\n",
        "\n",
        "    # For continuous, we'll use the stated label\n",
        "    # calculate median and use that as cutoff instead of 0.5 \n",
        "    median_val = 0.5\n",
        "    D[\"probs\"] = D[\"probs\"].apply(lambda x: float(x[1:-1]))\n",
        "    D[\"probs_binarized\"] = D[\"probs\"].apply(lambda x: 0 if x < median_val else 1)\n",
        "    D[\"label_binarized\"] = D[\"label\"]\n",
        "    print(median_val)\n",
        "    D2 = D.dropna()\n",
        "\n",
        "    demogColumns = [\n",
        "        \"Age_bin\", \"Gender_bin\", \n",
        "        \"Age_Gender\"\n",
        "    ]\n",
        "    DIs = []\n",
        "    FVs = []\n",
        "    demog_trues = []\n",
        "    D2[\"Gender_bin\"] = (D2[\"x1\"] == \"m\").astype(int)\n",
        "    D2[\"Age_bin\"] = (D2[\"x2\"] < 55).astype(int)\n",
        "    D2[\"Age_Gender\"] = (D2[\"Age_bin\"].astype(int) + D2[\"Gender_bin\"].astype(int)) / 2\n",
        "\n",
        "    # laplace smoothing to account for zeros\n",
        "    for dc in demogColumns:\n",
        "      \n",
        "      positive_predictions = D2[\"probs_binarized\"]==1\n",
        "      positive_gold = D2[\"label_binarized\"]==1\n",
        "      protected = D2[dc]==0\n",
        "      privileged = D2[dc] == 1\n",
        "      N = 2\n",
        "      alpha = 1\n",
        "\n",
        "      DI_numerator = (sum(positive_predictions & protected) + alpha) / (sum(protected) + N)\n",
        "      DI_denominator =  (sum(positive_predictions & privileged) + alpha) / (sum(privileged) + N)\n",
        "\n",
        "      ytrue_numerator = (sum(positive_gold & protected) + alpha) / (sum(protected) + N)\n",
        "      ytrue_denominator =  (sum(positive_gold & privileged) + alpha) / (sum(privileged) + N)\n",
        "      ypred_global = (sum(positive_predictions) + alpha) / (len(D2[dc]) + N)\n",
        "\n",
        "      try:\n",
        "        DI = DI_numerator / DI_denominator\n",
        "        ytrue = ytrue_numerator / ytrue_denominator\n",
        "        FV = np.abs(DI_numerator - ypred_global)\n",
        "      except:\n",
        "        DI=0\n",
        "        ytrue=0\n",
        "        FV=0\n",
        "      \n",
        "      if gold_ratio: \n",
        "          DI = DI / ytrue\n",
        "      print(dc)\n",
        "\n",
        "      \n",
        "      DIs.append(DI) \n",
        "      FVs.append(FV)\n",
        "      demog_trues.append((dc, ytrue))\n",
        "\n",
        "    # auc from sklearn\n",
        "    fpr, tpr, _ = metrics.roc_curve(D2[\"label_binarized\"], D2[\"probs\"], pos_label=1)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "    # other metrics: MSE, pearson, F1\n",
        "    mse = 0 \n",
        "    pearsonscore, prob = 0, 0\n",
        "    f1score = f1_score(D2[\"label_binarized\"], D2[\"probs_binarized\"])\n",
        "\n",
        "    return [mse, pearsonscore, f1score, auc] + DIs + FVs, demog_trues\n",
        "\n",
        "\n",
        "\n",
        "def get_results_MBTI(full_N, gold_ratio, models):\n",
        "    DIs, aucs, xaucs = [], [], []\n",
        "    task = []\n",
        "    dc_tracker = []\n",
        "    l = []\n",
        "    results = []\n",
        "    demog_trues = []\n",
        "\n",
        "    for basefname in models:\n",
        "        outs, dt = generatePlots_MBTI(basefname, full_N, gold_ratio)\n",
        "        if len(demog_trues) == 0:\n",
        "          demog_trues.extend(dt)\n",
        "        modelname = basefname.split(\"/\")[1]\n",
        "        results.append([modelname, m] + outs)\n",
        "\n",
        "    colnames = [\"model\", \n",
        "              \"DV\", \n",
        "              \"MSE\", \"Pearson R\", \"F1\",\n",
        "              \"AUC\", \n",
        "        \"DI_Age_bin\", \"DI_Gender_bin\", \n",
        "        \"DI_Age_Gender\", \n",
        "        \"FV_Age_bin\", \"FV_Gender_bin\",\n",
        "        \"FV_Age_Gender\", \n",
        "    ]\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame(results) \n",
        "    df.columns = colnames\n",
        "\n",
        "    return df, demog_trues\n",
        "\n",
        "def calculate_fairness_MBTI(infile, outfile, weighted=False, unionYN=False):\n",
        "\n",
        "  models = [infile]\n",
        "\n",
        "  output, demogs = get_results_MBTI(\n",
        "    unionYN,\n",
        "    weighted, \n",
        "    models\n",
        "  )\n",
        "\n",
        "  debiasing, wordlists = outfile.split(\"_\")[:2]\n",
        "  wordlists = wordlists.split(\".\")[0]\n",
        "  output[\"model\"] = outfile\n",
        "  output[\"adjustedDI\"] = weighted\n",
        "  output[\"debiasing\"] = debiasing\n",
        "  output[\"wordlists\"] = wordlists\n",
        "  output[\"fullN\"] = unionYN\n",
        "\n",
        "  output.to_csv(f\"fairness_output_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n",
        "  demog_df = pd.DataFrame(demogs, columns=[\"Demographic\", \"Y1_Ratio\"])\n",
        "  demog_df[\"model\"] = outfile\n",
        "  demog_df[\"ratio\"] = weighted\n",
        "\n",
        "  demog_df.to_csv(f\"y1_ratio_{outfile}_fullN_{unionYN}_goldRatio_{weighted}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzpfZKsLeDzq",
        "outputId": "59e12f80-ff92-4145-abbf-0cf6ece198c3"
      },
      "outputs": [],
      "source": [
        "taskNames = [\n",
        "            'MBTI_perceiving',\n",
        "             'MBTI_thinking',\n",
        "]\n",
        "\n",
        "debiasing = [\n",
        "    \"PT\",\n",
        "    \"PTD\"\n",
        "]\n",
        "\n",
        "tasks = [\n",
        "    \"Continuous\",\n",
        "    \"Binary\"\n",
        "]\n",
        "\n",
        "models = [\n",
        "    \"BERT\",\n",
        "    \"RoBERTa\"\n",
        "    \"CNN\"\n",
        "]\n",
        "\n",
        "\n",
        "for m in taskNames:\n",
        "    for d in debiasing:\n",
        "        for t in tasks:\n",
        "          for mm in models:\n",
        "            fname = f\"merged/{m}_{t}_{d}_{mm}_test.csv\"\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_F_F_test.csv\"\n",
        "            calculate_fairness_MBTI(fname, outname, weighted=False, unionYN=False)\n",
        "            outname = f\"{m}_{t}_{d}_{mm}_T_F_test.csv\"\n",
        "            calculate_fairness_MBTI(fname, outname, weighted=True, unionYN=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5aM0or61JgI"
      },
      "outputs": [],
      "source": [
        "# concatenate everything together\n",
        "!head -n 1 fairness_output_FIPI_Agreeable_Continuous_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_Psych_FIPI.csv\n",
        "!head -n 1 fairness_output_Hatespeech_Hatespeech_Binary_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_HS.csv\n",
        "!head -n 1 fairness_output_AskAPatient_AskAPatient_Continuous_PT_BERT_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_AAP.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq9NsWLuja1w"
      },
      "outputs": [],
      "source": [
        "!head -n 1 fairness_output_MBTI_perceiving_Binary_PT_CNN_F_F_test.csv_fullN_False_goldRatio_False.csv > results_bert_acl22_MBTI.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGMVaPgR2rLK",
        "outputId": "15c3541e-17bd-4732-c7ab-d9266c351f59"
      },
      "outputs": [],
      "source": [
        "!head results_bert_acl22*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y57gD072tgH"
      },
      "outputs": [],
      "source": [
        "!tail -n 1 -q fairness_output_FIPI* >> results_bert_acl22_Psych_FIPI.csv\n",
        "!tail -n 1 -q fairness_output_Psych* >> results_bert_acl22_Psych_FIPI.csv\n",
        "!tail -n 1 -q fairness_output_Ha* >> results_bert_acl22_HS.csv\n",
        "!tail -n 1 -q fairness_output_Ask* >> results_bert_acl22_AAP.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwsLk4I8jfo0"
      },
      "outputs": [],
      "source": [
        "!tail -n 1 -q fairness_output_MBTI* >> results_bert_acl22_MBTI.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ACL2022_fairness_metrics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
